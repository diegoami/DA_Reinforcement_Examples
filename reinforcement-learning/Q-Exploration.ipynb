{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Reinforcement Learning: Exploration Strategies\n",
    "This notebook contains implementations of various action-selections methods that can be used to encourage exploration during the learning process. To learn more about these methods, see the accompanying [Medium post](https://medium.com/p/d3a97b7cceaf/). Also see the interactive visualization: [here](http://awjuliani.github.io/exploration/index.html).\n",
    "\n",
    "For more reinforcment learning tutorials see:\n",
    "https://github.com/awjuliani/DeepRL-Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow.contrib.slim as slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-08 23:35:36,047] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Deep Q-Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 10000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self,size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer,size)),[size,5])\n",
    "    \n",
    "def updateTargetGraph(tfVars,tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the network itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Network():\n",
    "    def __init__(self):\n",
    "        #These lines establish the feed-forward part of the network used to choose actions\n",
    "        self.inputs = tf.placeholder(shape=[None,4],dtype=tf.float32)\n",
    "        self.Temp = tf.placeholder(shape=None,dtype=tf.float32)\n",
    "        self.keep_per = tf.placeholder(shape=None,dtype=tf.float32)\n",
    "\n",
    "        hidden = slim.fully_connected(self.inputs,64,activation_fn=tf.nn.tanh,biases_initializer=None)\n",
    "        hidden = slim.dropout(hidden,self.keep_per)\n",
    "        self.Q_out = slim.fully_connected(hidden,2,activation_fn=None,biases_initializer=None)\n",
    "        \n",
    "        self.predict = tf.argmax(self.Q_out,1)\n",
    "        self.Q_dist = tf.nn.softmax(self.Q_out/self.Temp)\n",
    "        \n",
    "        \n",
    "        #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions,2,dtype=tf.float32)\n",
    "        \n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Q_out, self.actions_onehot), reduction_indices=1)\n",
    "        \n",
    "        self.nextQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        loss = tf.reduce_sum(tf.square(self.nextQ - self.Q))\n",
    "        trainer = tf.train.GradientDescentOptimizer(learning_rate=0.0005)\n",
    "        self.updateModel = trainer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set learning parameters\n",
    "exploration = \"e-greedy\" #Exploration method. Choose between: greedy, random, e-greedy, boltzmann, bayesian.\n",
    "y = .99 #Discount factor.\n",
    "num_episodes = 20000 #Total number of episodes to train network for.\n",
    "tau = 0.001 #Amount to update target network at each step.\n",
    "batch_size = 32 #Size of training batch\n",
    "startE = 1 #Starting chance of random action\n",
    "endE = 0.1 #Final chance of random action\n",
    "anneling_steps = 200000 #How many steps of training to reduce startE to endE.\n",
    "pre_train_steps = 50000 #Number of steps used before training updates begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/diego/anaconda3/envs/dsretreat-s/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-08 23:35:40,167] From /home/diego/anaconda3/envs/dsretreat-s/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward: 20.65 Total Steps: 2079 e: 1\n",
      "Mean Reward: 21.21 Total Steps: 4200 e: 1\n",
      "Mean Reward: 22.27 Total Steps: 6427 e: 1\n",
      "Mean Reward: 21.83 Total Steps: 8610 e: 1\n",
      "Mean Reward: 23.18 Total Steps: 10928 e: 1\n",
      "Mean Reward: 21.34 Total Steps: 13062 e: 1\n",
      "Mean Reward: 24.78 Total Steps: 15540 e: 1\n",
      "Mean Reward: 20.42 Total Steps: 17582 e: 1\n",
      "Mean Reward: 22.87 Total Steps: 19869 e: 1\n",
      "Mean Reward: 21.06 Total Steps: 21975 e: 1\n",
      "Mean Reward: 23.57 Total Steps: 24332 e: 1\n",
      "Mean Reward: 22.14 Total Steps: 26546 e: 1\n",
      "Mean Reward: 24.23 Total Steps: 28969 e: 1\n",
      "Mean Reward: 22.36 Total Steps: 31205 e: 1\n",
      "Mean Reward: 22.46 Total Steps: 33451 e: 1\n",
      "Mean Reward: 22.97 Total Steps: 35748 e: 1\n",
      "Mean Reward: 23.76 Total Steps: 38124 e: 1\n",
      "Mean Reward: 22.58 Total Steps: 40382 e: 1\n",
      "Mean Reward: 22.91 Total Steps: 42673 e: 1\n",
      "Mean Reward: 22.92 Total Steps: 44965 e: 1\n",
      "Mean Reward: 22.79 Total Steps: 47244 e: 1\n",
      "Mean Reward: 22.08 Total Steps: 49452 e: 1\n",
      "Mean Reward: 21.57 Total Steps: 51609 e: 0.9927640000000597\n",
      "Mean Reward: 23.74 Total Steps: 53983 e: 0.9820810000001479\n",
      "Mean Reward: 23.49 Total Steps: 56332 e: 0.9715105000002351\n",
      "Mean Reward: 24.67 Total Steps: 58799 e: 0.9604090000003267\n",
      "Mean Reward: 21.0 Total Steps: 60899 e: 0.9509590000004047\n",
      "Mean Reward: 20.54 Total Steps: 62953 e: 0.941716000000481\n",
      "Mean Reward: 23.11 Total Steps: 65264 e: 0.9313165000005668\n",
      "Mean Reward: 24.56 Total Steps: 67720 e: 0.920264500000658\n",
      "Mean Reward: 21.73 Total Steps: 69893 e: 0.9104860000007386\n",
      "Mean Reward: 23.11 Total Steps: 72204 e: 0.9000865000008245\n",
      "Mean Reward: 25.46 Total Steps: 74750 e: 0.888629500000919\n",
      "Mean Reward: 24.05 Total Steps: 77155 e: 0.8778070000010083\n",
      "Mean Reward: 24.32 Total Steps: 79587 e: 0.8668630000010986\n",
      "Mean Reward: 23.69 Total Steps: 81956 e: 0.8562025000011866\n",
      "Mean Reward: 23.8 Total Steps: 84336 e: 0.845492500001275\n",
      "Mean Reward: 24.74 Total Steps: 86810 e: 0.8343595000013668\n",
      "Mean Reward: 24.94 Total Steps: 89304 e: 0.8231365000014594\n",
      "Mean Reward: 24.01 Total Steps: 91705 e: 0.8123320000015486\n",
      "Mean Reward: 22.48 Total Steps: 93953 e: 0.8022160000016321\n",
      "Mean Reward: 25.49 Total Steps: 96502 e: 0.7907455000017267\n",
      "Mean Reward: 21.91 Total Steps: 98693 e: 0.7808860000018081\n",
      "Mean Reward: 21.03 Total Steps: 100796 e: 0.7714225000018862\n",
      "Mean Reward: 26.1 Total Steps: 103406 e: 0.7596775000019831\n",
      "Mean Reward: 25.31 Total Steps: 105937 e: 0.7482880000020771\n",
      "Mean Reward: 24.42 Total Steps: 108379 e: 0.7372990000021677\n",
      "Mean Reward: 26.61 Total Steps: 111040 e: 0.7253245000022666\n",
      "Mean Reward: 27.47 Total Steps: 113787 e: 0.7129630000023686\n",
      "Mean Reward: 24.47 Total Steps: 116234 e: 0.7019515000024594\n",
      "Mean Reward: 26.31 Total Steps: 118865 e: 0.6901120000025571\n",
      "Mean Reward: 28.92 Total Steps: 121757 e: 0.6770980000026645\n",
      "Mean Reward: 25.31 Total Steps: 124288 e: 0.6657085000027585\n",
      "Mean Reward: 26.38 Total Steps: 126926 e: 0.6538375000028565\n",
      "Mean Reward: 27.98 Total Steps: 129724 e: 0.6412465000029604\n",
      "Mean Reward: 25.38 Total Steps: 132262 e: 0.6298255000030546\n",
      "Mean Reward: 25.49 Total Steps: 134811 e: 0.6183550000031492\n",
      "Mean Reward: 27.98 Total Steps: 137609 e: 0.6057640000032531\n",
      "Mean Reward: 31.09 Total Steps: 140718 e: 0.5917735000033686\n",
      "Mean Reward: 29.39 Total Steps: 143657 e: 0.5785480000034777\n",
      "Mean Reward: 30.64 Total Steps: 146721 e: 0.5647600000035915\n",
      "Mean Reward: 31.82 Total Steps: 149903 e: 0.5504410000037097\n",
      "Mean Reward: 28.07 Total Steps: 152710 e: 0.5378095000038139\n",
      "Mean Reward: 31.4 Total Steps: 155850 e: 0.5236795000039305\n",
      "Mean Reward: 28.29 Total Steps: 158679 e: 0.5109490000040355\n",
      "Mean Reward: 29.45 Total Steps: 161624 e: 0.49769650000411647\n",
      "Mean Reward: 29.51 Total Steps: 164575 e: 0.48441700000406224\n",
      "Mean Reward: 28.38 Total Steps: 167413 e: 0.4716460000040101\n",
      "Mean Reward: 33.85 Total Steps: 170798 e: 0.45641350000394787\n",
      "Mean Reward: 37.32 Total Steps: 174530 e: 0.4396195000038793\n",
      "Mean Reward: 42.97 Total Steps: 178827 e: 0.4202830000038003\n",
      "Mean Reward: 49.75 Total Steps: 183802 e: 0.3978955000037089\n",
      "Mean Reward: 61.86 Total Steps: 189988 e: 0.3700585000035952\n",
      "Mean Reward: 108.72 Total Steps: 200860 e: 0.3211345000033954\n",
      "Mean Reward: 149.44 Total Steps: 215804 e: 0.25388650000312074\n",
      "Mean Reward: 159.45 Total Steps: 231749 e: 0.1821340000032463\n",
      "Mean Reward: 160.7 Total Steps: 247819 e: 0.10981900000335018\n",
      "Mean Reward: 165.29 Total Steps: 264348 e: 0.09999550000334036\n",
      "Mean Reward: 152.33 Total Steps: 279581 e: 0.09999550000334036\n",
      "Mean Reward: 134.48 Total Steps: 293029 e: 0.09999550000334036\n",
      "Mean Reward: 145.68 Total Steps: 307597 e: 0.09999550000334036\n",
      "Mean Reward: 148.83 Total Steps: 322480 e: 0.09999550000334036\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "q_net = Q_Network()\n",
    "target_net = Q_Network()\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "trainables = tf.trainable_variables()\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "jMeans = []\n",
    "rList = []\n",
    "rMeans = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    updateTarget(targetOps,sess)\n",
    "    e = startE\n",
    "    stepDrop = (startE - endE)/anneling_steps\n",
    "    total_steps = 0\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        while j < 999:\n",
    "            j+=1\n",
    "            if exploration == \"greedy\":\n",
    "                #Choose an action with the maximum expected value.\n",
    "                a,allQ = sess.run([q_net.predict,q_net.Q_out],feed_dict={q_net.inputs:[s],q_net.keep_per:1.0})\n",
    "                a = a[0]\n",
    "            if exploration == \"random\":\n",
    "                #Choose an action randomly.\n",
    "                a = env.action_space.sample()\n",
    "            if exploration == \"e-greedy\":\n",
    "                #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "                if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                    a = env.action_space.sample()\n",
    "                else:\n",
    "                    a,allQ = sess.run([q_net.predict,q_net.Q_out],feed_dict={q_net.inputs:[s],q_net.keep_per:1.0})\n",
    "                    a = a[0]\n",
    "            if exploration == \"boltzmann\":\n",
    "                #Choose an action probabilistically, with weights relative to the Q-values.\n",
    "                Q_d,allQ = sess.run([q_net.Q_dist,q_net.Q_out],feed_dict={q_net.inputs:[s],q_net.Temp:e,q_net.keep_per:1.0})\n",
    "                a = np.random.choice(Q_d[0],p=Q_d[0])\n",
    "                a = np.argmax(Q_d[0] == a)\n",
    "            if exploration == \"bayesian\":\n",
    "                #Choose an action using a sample from a dropout approximation of a bayesian q-network.\n",
    "                a,allQ = sess.run([q_net.predict,q_net.Q_out],feed_dict={q_net.inputs:[s],q_net.keep_per:(1-e)+0.1})\n",
    "                a = a[0]\n",
    "                \n",
    "            #Get new state and reward from environment\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            myBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[1,5]))\n",
    "            \n",
    "            if e > endE and total_steps > pre_train_steps:\n",
    "                e -= stepDrop\n",
    "            \n",
    "            if total_steps > pre_train_steps and total_steps % 5 == 0:\n",
    "                #We use Double-DQN training algorithm\n",
    "                trainBatch = myBuffer.sample(batch_size)\n",
    "                Q1 = sess.run(q_net.predict,feed_dict={q_net.inputs:np.vstack(trainBatch[:,3]),q_net.keep_per:1.0})\n",
    "                Q2 = sess.run(target_net.Q_out,feed_dict={target_net.inputs:np.vstack(trainBatch[:,3]),target_net.keep_per:1.0})\n",
    "                end_multiplier = -(trainBatch[:,4] - 1)\n",
    "                doubleQ = Q2[range(batch_size),Q1]\n",
    "                targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n",
    "                _ = sess.run(q_net.updateModel,feed_dict={q_net.inputs:np.vstack(trainBatch[:,0]),q_net.nextQ:targetQ,q_net.keep_per:1.0,q_net.actions:trainBatch[:,1]})\n",
    "                updateTarget(targetOps,sess)\n",
    "\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            total_steps += 1\n",
    "            if d == True:\n",
    "                break\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "        if i % 100 == 0 and i != 0:\n",
    "            r_mean = np.mean(rList[-100:])\n",
    "            j_mean = np.mean(jList[-100:])\n",
    "            if exploration == 'e-greedy':\n",
    "                print(\"Mean Reward: \" + str(r_mean) + \" Total Steps: \" + str(total_steps) + \" e: \" + str(e))\n",
    "            if exploration == 'boltzmann':\n",
    "                print(\"Mean Reward: \" + str(r_mean) + \" Total Steps: \" + str(total_steps) + \" t: \" + str(e))\n",
    "            if exploration == 'bayesian':\n",
    "                print(\"Mean Reward: \" + str(r_mean) + \" Total Steps: \" + str(total_steps) + \" p: \" + str(e))\n",
    "            if exploration == 'random' or exploration == 'greedy':\n",
    "                print(\"Mean Reward: \" + str(r_mean) + \" Total Steps: \" + str(total_steps))\n",
    "            rMeans.append(r_mean)\n",
    "            jMeans.append(j_mean)\n",
    "print(\"Percent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some statistics on network performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(jMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
