{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Introduction to environment \n",
    "This time we will teach our self driving car to drive us home (orange node). We have to be careful though as some streets are under construction (grey node) and we don’t want our car crashing into it.<br>\n",
    "<br>\n",
    "As you can see we have streets numbered from 0 to 8. This gives us 9 unique **states** (streets). At any given time, our car (agent) can be in one of this 9 states. State 8 is our Goal, also called as a **terminal state**.<br>\n",
    "<br>\n",
    "Our car can go left, right, up and down. To put it differently, our agent can take four different **actions**. We write it as: a ∈ A{up,down,left,right}<br>\n",
    "<br>\n",
    "The agent receives a **reward 10 for reaching the terminal state**, other than that there are no rewards.\n",
    "![GirdWorld](http://i.imgur.com/C1fj5ZE.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "#import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-04 10:23:33,716] Making new env: FrozenLake-v0\n"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env_list = ['FrozenLake-v0','Taxi-v2']\n",
    "env = gym.make(env_list[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Q-learning and Q-table \n",
    "\n",
    "Now we’ll create a matrix, “Q” also called as Q-table, this will be the brain of our agent. The matrix Q is initialized to zero, as agent starts out knowing nothing (just like John Snow;)). It updates Q-table with new values for state-action pair, as it learns. Here is the formula to calculate the Q[state, action] <br> \n",
    "<br>\n",
    "Q[s, a] = Q[s, a] + alpha*(R + gamma*Max[Q(s’, A)] - Q[s, a]) <br>\n",
    "<br>\n",
    "Where;<br>\n",
    "alpha is the **learning rate**,<br>\n",
    "gamma is the **discount factor**. It quantifies how much importance we give for future rewards. It’s also handy to approximate the noise in future rewards. Gamma varies from 0 to 1. If Gamma is closer to zero, the agent will tend to consider only immediate rewards. If Gamma is closer to one, the agent will consider future rewards with greater weight, willing to delay the reward.<br>\n",
    "Max[Q(s’, A)] gives **a maximum value of Q for all possible actions in the next state**.<br>\n",
    "<br>\n",
    "The Agent explores different ‘state-action’ combinations till it reaches the goal or falls into the hole. We will call each of this explorations an **episode**. Each time the agent arrives at goal or is terminated, we start with next episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Some easy maths to demo how Q-table is updated:\n",
    "![Q-table](http://i.imgur.com/WI0Dele.png)\n",
    "take learning rate (alpha) as 0.5 & discount factor (gamma) as 0.9 <br>\n",
    "Q[s, a] = Q[s, a] + alpha*(R + gamma*Max[Q(s’, A)] — Q[s, a])<br>\n",
    "Early Episodes<br>\n",
    "Q[3,L] = Q[3,L]+0.5*(10+0.9*Max[Q(8,U),Q(8,D),Q(8,R),Q(8,L)]-Q(3,L)) <br>\n",
    "Q[3,L] = 0 + 0.5 * (10 + 0.9 * Max [0, 0, 0, 0] -0)<br>\n",
    "Q[3,L] = 5, Similarly Q[6,D] = 5<br>\n",
    "Next Episodes<br>\n",
    "Q[2,L] = Q[2,L]+0.5*(0+0.9*Max[Q(6,U),Q(6,D),Q(6,R),Q(6,L)]-Q(2,L))<br>\n",
    "Q[2,L] = 0 + 0.5 * (0 + 0.9 * Max [0, 5, 0, 0] -0)<br>\n",
    "Q[2,L] = 2.25, Similarly Q[2,D] = 2.25 and Q[7,L] = 2.25<br>\n",
    "Eventually<br>\n",
    "Q[1,D] = 1.0125 and Q[0,L] = 0.455625<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exploration and Exploitation — Epsilon (ε)\n",
    "As agent begins the learning, we would want it to take random actions to explore more paths. But as the agent gets better, the Q-function converges to more consistent Q-values. Now we would like our agent to exploit paths with highest Q-value i.e takes greedy actions. This is where epsilon comes in.<br> \n",
    "<br> \n",
    "The agent takes random actions for probability ε and greedy action for probability (1-ε). <br> \n",
    "<br> \n",
    "Google DeepMind used a decaying ε-greedy action selection. Where ε decays over time from 1 to 0.1 — in the beginning, the system makes completely random moves to explore the state space maximally, and then it settles down to a fixed exploration rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Pseudo Code\n",
    "![Q-learning](http://i.imgur.com/EeZcNeR.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def q_learning(env, num_episodes, alpha=0.85, discount_factor=0.99):\n",
    "    \"\"\"\n",
    "    Q learning algorithm, off-polics TD control. Finds optimal gready policies\n",
    "    Args:\n",
    "    env: Given environment to solve\n",
    "    num_episodes: Number of episodes to learn\n",
    "    alpha: learning rate\n",
    "    discount factor: weight/importance given to future rewards\n",
    "    epsilon: probability of taking random action. \n",
    "             We are using decaying epsilon, \n",
    "             i.e high randomness at beginning and low towards end\n",
    "    Returns:\n",
    "    Optimal Q\n",
    "    \"\"\"\n",
    "     \n",
    "    # decaying epsilon, i.e we will divide num of episodes passed\n",
    "    epsilon = 1.0\n",
    "    # create a numpy array filled with zeros \n",
    "    # rows = number of observations & cols = possible actions\n",
    "    Q = np.zeros([env.observation_space.n, env.action_space.n]) \n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "            # reset the env\n",
    "            state = env.reset()\n",
    "            # itertools.count() has similar to 'while True:'\n",
    "            for t in itertools.count():\n",
    "                # generate a random num between 0 and 1 e.g. 0.35, 0.73 etc..\n",
    "                # if the generated num is smaller than epsilon, we follow exploration policy \n",
    "                if np.random.random() <= epsilon:\n",
    "                    # select a random action from set of all actions\n",
    "                    action = env.action_space.sample()\n",
    "                # if the generated num is greater than epsilon, we follow exploitation policy\n",
    "                else:\n",
    "                    # select an action with highest value for current state\n",
    "                    action = np.argmax(Q[state, :])\n",
    "                \n",
    "                # apply selected action, collect values for next_state and reward\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                \n",
    "                # Calculate the Q-learning target value\n",
    "                Q_target = reward + discount_factor*np.max(Q[next_state,:])\n",
    "                # Calculate the difference/error between target and current Q\n",
    "                Q_delta = Q_target - Q[state,action]\n",
    "                # Update the Q table, alpha is the learning rate\n",
    "                Q[state, action] = Q[state, action] + (alpha * Q_delta)\n",
    "                \n",
    "                # break if done, i.e. if end of this episode\n",
    "                if done:\n",
    "                    break\n",
    "                # make the next_state into current state as we go for next iteration\n",
    "                state = next_state\n",
    "            # gradualy decay the epsilon\n",
    "            if epsilon > 0.1:\n",
    "                epsilon -= 1.0/num_episodes\n",
    "    \n",
    "    return Q    # return optimal Q\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def test_algorithm(env, Q):\n",
    "    \"\"\"\n",
    "    Test script for Q function\n",
    "    Args:\n",
    "    env: Given environment to test Q function\n",
    "    Q: Q function to verified\n",
    "    Returns:\n",
    "    Total rewards for one episode\n",
    "    \"\"\"\n",
    "    \n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    while True:\n",
    "        # selection the action with highest values i.e. best action\n",
    "        action = np.argmax(Q[state, :])\n",
    "        # apply selected action\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        # render environment\n",
    "        env.render()\n",
    "        # calculate total reward\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            print(total_reward)\n",
    "            break\n",
    "            \n",
    "        state = next_state\n",
    "    \n",
    "    return total_reward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Q = q_learning(env, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.81601034e-01   5.48947196e-01   5.71618900e-01   5.80961970e-01]\n",
      " [  7.73473571e-02   2.74745056e-02   5.20363800e-01   6.90664599e-01]\n",
      " [  2.27252795e-01   4.96515417e-01   1.37152480e-01   3.31289754e-01]\n",
      " [  4.73595482e-02   3.74819510e-02   3.51660510e-02   4.73937444e-01]\n",
      " [  5.96029365e-01   9.61543241e-02   5.49391393e-05   1.20234534e-01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  2.13649665e-02   1.68729101e-02   2.95534048e-04   3.34253584e-05]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  1.17522777e-01   1.80769975e-02   6.30194144e-01   7.58542005e-01]\n",
      " [  2.78574817e-03   8.55462125e-01   1.05827056e-01   3.04137863e-03]\n",
      " [  8.51346298e-01   1.39887216e-01   1.15799949e-02   1.10476723e-02]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  1.43397267e-01   1.40706256e-01   9.39220229e-01   5.50304369e-01]\n",
      " [  8.78885486e-01   8.84034933e-01   9.95306518e-01   8.39102516e-01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_algorithm(env, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Further resources\n",
    "[Reinforcement Learning: An Introduction](http://people.inf.elte.hu/lorincz/Files/RL_2006/SuttonBook.pdf) — Chapter 6: Temporal-Difference Learning <br>\n",
    "[David Silver’s RL Course Lecture 4](https://www.youtube.com/watch?v=PnHCvfgC_ZA) — Model-Free Prediction <br>\n",
    "[David Silver’s RL Course Lecture 5](https://www.youtube.com/watch?v=0g4j2k_Ggc4) — Model-Free Control <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I hope this tutorial has been helpful to those new to Q-learning and TD-reinforcement learning!<br>\n",
    "If you’d like to follow my writing on Reinforcement Learning, follow me on Medium [@Shreyas Gite](https://medium.com/@shreyas.gite), or on twitter [@shreyasgite](https://twitter.com/shreyasgite).<br>\n",
    "Feel free to write to me for any questions or suggestions :)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
